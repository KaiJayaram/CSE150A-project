{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# read data from pandas\n",
    "df = pd.read_json('goodreads_interactions_poetry.json', lines=True)\n",
    "\n",
    "# parse into matrix\n",
    "book_id = df['book_id'].tolist()\n",
    "user_id = df['user_id'].tolist()\n",
    "ratings = df['rating'].tolist()\n",
    "interactions = [[x,y,z] for (x,y,z) in zip(user_id, book_id, ratings)]\n",
    "\n",
    "# convert to dictionary\n",
    "user_dict = {}\n",
    "for user, book, rating in interactions:\n",
    "    if user not in user_dict:\n",
    "        user_dict[user] = {}\n",
    "    user_dict[user][book] = rating\n",
    "X = []\n",
    "for user in user_dict.keys():\n",
    "    X.append(user_dict[user])\n",
    "    \n",
    "num_itters = 128\n",
    "log_probs = []\n",
    "\n",
    "n = 1000\n",
    "\n",
    "# save x before truncating\n",
    "with open('poetry_full_users.pickle', 'wb') as handle:\n",
    "    pickle.dump(X, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# reduce data size to n and map book ids to nicer numbers\n",
    "X = X[:n]\n",
    "new_X = []\n",
    "book_map = {}\n",
    "num_books = 0\n",
    "for user in X:\n",
    "    books = {}\n",
    "    for book in user.keys():\n",
    "        if book not in book_map:\n",
    "            book_map[book] = num_books\n",
    "            num_books+=1\n",
    "        books[book_map[book]] = user[book]\n",
    "    new_X.append(books)\n",
    "X = new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EM Algorithm\n",
    "\n",
    "k = num_books # number of books\n",
    "ny = 5 # categories\n",
    "# initialize prjyi with random values\n",
    "prjyi = np.random.random((k, ny))\n",
    "py = [0.4, 0.2, 0.2, 0.1, 0.1] # start with semi random valuse adding to 1\n",
    "for itter in range(num_itters+1):\n",
    "    if(itter != 0):\n",
    "        rhoit = [[0] * ny for l in range(n)]\n",
    "        for t in range(n):\n",
    "            probs = []\n",
    "            for i in range(ny):\n",
    "                temp = 1.0\n",
    "                for j in range(k):\n",
    "                    if j in X[t]:\n",
    "                        if X[t][j] >=2:\n",
    "                            temp *= prjyi[j][i]\n",
    "                        else:\n",
    "                            temp *= (1-prjyi[j][i])\n",
    "                probs.append(temp * py[i])\n",
    "            psum = sum(probs)\n",
    "            rhoit[t] = [l / psum for l in probs]\n",
    "            \n",
    "        py_new = [0.0] * ny\n",
    "        for t in range(n):\n",
    "            for i in range(ny):\n",
    "                py_new[i] += rhoit[t][i]\n",
    "        py_new = [l / n for l in py_new]\n",
    "\n",
    "        # New PRJYI\n",
    "        prjyi_new = [[0.0] * ny for l in range(k)]\n",
    "        for i in range(ny):\n",
    "            for j in range(k):\n",
    "                for t in range(n):\n",
    "                    if j not in X[t]:\n",
    "                        # Not seen case\n",
    "                        prjyi_new[j][i] += rhoit[t][i] * prjyi[j][i]\n",
    "                    else:\n",
    "                        # Seen case\n",
    "                        prjyi_new[j][i] += rhoit[t][i] * (1 if X[t][j] >= 3 else 0)\n",
    "                prjyi_new[j][i] /= sum([row[i] for row in rhoit])\n",
    "\n",
    "\n",
    "        ## Do update\n",
    "        py = py_new\n",
    "        print(py)\n",
    "        prjyi = prjyi_new\n",
    "\n",
    "    print(\"finished updates\")\n",
    "    ## Get Log Probabilities\n",
    "    log_prob = 0.0\n",
    "    for t in range(n):\n",
    "        prob = 0.0\n",
    "        for i in range(ny):\n",
    "            temp = 1.0\n",
    "            for j in range(k):\n",
    "                if (j in X[t]):\n",
    "                    # has seen\n",
    "                    if X[t][j] >=2:\n",
    "                        temp *= prjyi[j][i]\n",
    "                    else:\n",
    "                        temp *= 1-prjyi[j][i]\n",
    "            prob += (py[i] * temp)\n",
    "        \n",
    "        log_prob += np.log(prob)\n",
    "    print(itter)\n",
    "    log_prob /= n\n",
    "    log_probs.append(log_prob)\n",
    "\n",
    "with open('prjyi.pickle', 'wb') as handle:\n",
    "    pickle.dump(prjyi, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('py.pickle', 'wb') as handle:\n",
    "    pickle.dump(py, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "data_out = []\n",
    "for i in range(len(X)):\n",
    "    line = [np.argmax(np.array(rhoit[i]))]\n",
    "    line.append(X[i])\n",
    "    data_out.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data\n",
    "with open('em_result.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_out, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('em_result.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#categories are 0,1,2,3,4\n",
    "number_of_books = 3561\n",
    "books = []\n",
    "for i in range(3562):\n",
    "    books.append(i)\n",
    "    \n",
    "#recommendation for person in category 0\n",
    "\n",
    "#creating a list with only in the people in category 0\n",
    "shopper_0 = list()\n",
    "shopper_1 = list()\n",
    "shopper_2 = list()\n",
    "shopper_3 = list()\n",
    "shopper_4 = list()\n",
    "shopper_0_books_ratings = list()\n",
    "for i in data:\n",
    "    if i[0] == 0:\n",
    "        shopper_0.append(i[1])\n",
    "    if i[0] == 1:\n",
    "        shopper_1.append(i[1])\n",
    "    if i[0] == 2:\n",
    "        shopper_2.append(i[1])\n",
    "    if i[0] == 3:\n",
    "        shopper_3.append(i[1])\n",
    "    if i[0] == 4:\n",
    "        shopper_4.append(i[1])\n",
    "\n",
    "## map book ids to book names\n",
    "book_mapping = {}\n",
    "df = pd.read_json('goodreads_books_poetry.json', lines=True)\n",
    "book_ids = df['book_id'].tolist()\n",
    "titles = df['title'].tolist()\n",
    "title_map = [[book_id,title] for (book_id,title) in zip(book_ids, titles)]\n",
    "for book_id, title in title_map:\n",
    "    if book_id in book_map:\n",
    "        book_mapping[book_map[book_id]] = title\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# epsilon greedy approach\n",
    "number_of_books = 3562\n",
    "def epsilon_greedy_train(shopper_list, num_books):\n",
    "    averages = []\n",
    "    for i in range(num_books):\n",
    "        averages.append([0,0])\n",
    "    \n",
    "    for ratings in shopper_list:\n",
    "        for key in ratings.keys():\n",
    "            averages[key][1] *= averages[key][0]\n",
    "            averages[key][1] += ratings[key]\n",
    "            averages[key][0] += 1\n",
    "            averages[key][1] /= averages[key][0]\n",
    "    return averages\n",
    "def calculate_epsilon_greedy(current_averages, num_books, epsilon, book_mapping, seen_books):\n",
    "    should_explore = random.random()\n",
    "    if should_explore < epsilon:\n",
    "        book_choice = book_mapping[random.choice(range(num_books))]\n",
    "        while book_choice in seen_books:\n",
    "            book_choice = book_mapping[random.choice(range(num_books))]\n",
    "        return book_choice\n",
    "    else:\n",
    "        best = -1\n",
    "        best_idx = -1\n",
    "        for i in range(len(current_averages)):\n",
    "            if book_mapping[i] not in seen_books and current_averages[i][1] > best:\n",
    "                best = current_averages[i][1]\n",
    "                best_idx = i\n",
    "        return book_mapping[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sample 10 recomendations for each category\n",
    "recomendations = []\n",
    "epsilon = 0.1\n",
    "shoppers = [shopper_0, shopper_1, shopper_2 ,shopper_3 , shopper_4]\n",
    "for i in range(5):\n",
    "    recomendations.append(set())\n",
    "    averages = epsilon_greedy_train(shoppers[i], number_of_books)\n",
    "    seen_books = set()\n",
    "    while(len(recomendations[i]) < 10):\n",
    "        rec = calculate_epsilon_greedy(averages, number_of_books, epsilon, book_mapping, seen_books)\n",
    "        seen_books.add(rec)\n",
    "        recomendations[i].add(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in saved data\n",
    "with open('poetry_full_users.pickle', 'rb') as handle:\n",
    "    data_new = pickle.load(handle)\n",
    "test_user_list = []\n",
    "\n",
    "# get the users that have reviewed at least 3 books from our book set to test with\n",
    "for i in data_new:\n",
    "    counter_to_include = 0\n",
    "    for j in i.keys():\n",
    "        if j in book_map:\n",
    "            counter_to_include += 1\n",
    "    if counter_to_include > 3:\n",
    "        test_user_list.append(i)\n",
    "new_users = test_user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in saved data\n",
    "with open('poetry_full_users_test.pickle', 'wb') as handle:\n",
    "    pickle.dump(new_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "new_users = new_users[-1000:]\n",
    "\n",
    "#map books in new_users to books previously\n",
    "new_user_mapping = []\n",
    "for i in new_users:\n",
    "    user_books = {}\n",
    "    for b,r in i.items():\n",
    "        if b in book_map.keys():\n",
    "            user_books[book_map[b]] = r\n",
    "    new_user_mapping.append(user_books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate probability of being in each cagtegory for each user\n",
    "\n",
    "rhoit_new = [[0] * ny for l in range(n)]\n",
    "for t in range(n):\n",
    "    probs = []\n",
    "    for i in range(ny):\n",
    "        temp = 1.0\n",
    "        for j in range(k):\n",
    "            if j in new_user_mapping[t]:\n",
    "                if new_user_mapping[t][j] >=2:\n",
    "                    temp *= prjyi[j][i]\n",
    "                else:\n",
    "                    temp *= (1-prjyi[j][i])\n",
    "        probs.append(temp * py[i])\n",
    "        psum = sum(probs)\n",
    "        rhoit_new[t] = [l / psum for l in probs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign each of the test users a category\n",
    "data_out_new = []\n",
    "for i in range(len(new_user_mapping)):\n",
    "    line = [np.argmax(np.array(rhoit_new[i]))]\n",
    "    line.append(new_user_mapping[i])\n",
    "    data_out_new.append(line)\n",
    "\n",
    "# create data set of user categories and books they have reviewed\n",
    "data_out_new_mapping = []\n",
    "for i in data_out_new:\n",
    "    new_list = []\n",
    "    new_list.append(i[0])\n",
    "    user_books_new = {}\n",
    "    for b,r in i[1].items():\n",
    "        user_books_new[book_mapping[b]] = r\n",
    "    new_list.append(user_books_new)\n",
    "    data_out_new_mapping.append(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop cases where no books read in current books selection\n",
    "# Analyze the percentage of good recomendations we made \n",
    "#(# of recomendations that a new user actually reviwed positively / total # of recomendations a user reviweed\n",
    "num = 0\n",
    "denom = 0\n",
    "for user in data_out_new_mapping:\n",
    "    user_test_cat = user[0]\n",
    "    user_test_books = user[1]\n",
    "    for test_book_name,test_book_rating in user_test_books.items():\n",
    "        if test_book_name in recomendations[user_test_cat]:\n",
    "            denom += 1\n",
    "            if test_book_rating >=3 :\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate % of users who were recomended a book that they actually reviewed\n",
    "denom = 0\n",
    "for user in data_out_new_mapping:\n",
    "    denom += 1\n",
    "    user_test_cat = user[0]\n",
    "    user_test_books = user[1]\n",
    "    for test_book_name,test_book_rating in user_test_books.items():\n",
    "        if test_book_name in recomendations[user_test_cat]:\n",
    "\n",
    "            num += 1\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
